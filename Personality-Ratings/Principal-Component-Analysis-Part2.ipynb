{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to run the ML algorithms on the reduced dimensions generated by PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "LOG_FILENAME = 'Personality_Ratings_PCA.log'\n",
    "logging.basicConfig(filename=LOG_FILENAME,level=logging.INFO)\n",
    "from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "from sklearn.pipeline import make_pipeline,Pipeline\n",
    "from sklearn.model_selection import KFold,GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load MLOperationsUtilities.py\n",
    "def readDataFromCsv(file):\n",
    "    import pandas as pd\n",
    "    print (\"Reading the file from: \",file)\n",
    "    df = pd.read_csv(file)\n",
    "    return df\n",
    "\n",
    "def loadDataset(paths=['../datasets/files_generated/Personality/study1_features_data.csv',\n",
    "                      '../datasets/files_generated/Personality/study2_features_data.csv'],target='Neuroticism'):\n",
    "    for path in paths:\n",
    "        if 'study1' in path:\n",
    "            df = readDataFromCsv(path)\n",
    "            df= df.select_dtypes (['int64','float64']).drop(['VP','age','user_id'],axis=1)\n",
    "            print('The shape of the data  currently in study1: ',df.shape)\n",
    "            X_study1,y_study1= df.drop(['Neuroticism', 'Extraversion', \n",
    "                                        'Openness', 'Agreeableness','Conscientiousness'],axis=1),df[target]\n",
    "        elif 'study2'in path:\n",
    "            df = readDataFromCsv(path)\n",
    "            df = df.select_dtypes(['int64','float64']).drop(['user_id','UserId','VP','Age','Handedness_Score'],axis=1)\n",
    "            print('The shape of the data  currently in study2: ',df.shape)\n",
    "            X_study2,y_study2=df.drop(['Neuroticism', 'Extraversion', 'Openness', 'Agreeableness','Conscientiousness'],axis=1),df[target]\n",
    "        else:\n",
    "            df = pd.read_csv(path,index_col=0)\n",
    "            X,y=df.drop(['Neuroticism', 'Extraversion', \n",
    "                                        'Openness', 'Agreeableness','Conscientiousness','user_id'],axis=1),df[target]\n",
    "    # concat both the studies\n",
    "    if(len(paths)>1):\n",
    "        X = pd.concat([X_study1,X_study2])\n",
    "        y= pd.concat([y_study1,y_study2])\n",
    "    print('The shape of the data after concating both the studies {}'.format(X.shape))\n",
    "    print('The shape of the target after concating both the studies {}'.format(y.shape))\n",
    "    assert df.isnull().values.any()==False, 'Please check for null values'\n",
    "    df_result={'data':X,'target':y}\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths=['../datasets/files_generated/Personality/combined_features_data_out_mahalanobis_transformedDistributions.csv']\n",
    "paths=['../datasets/files_generated/Personality/study1_features_data_out_mahalanobis.csv',\n",
    "                      '../datasets/files_generated/Personality/study2_features_data_out_mahalanobis.csv']\n",
    "# paths=['../datasets/files_generated/Personality/combined_features_data_out_mahalanobis.csv']\n",
    "\n",
    "target = 'Neuroticism'\n",
    "filename='Tables/PCA_alltargets_mahalanobis_3PC.csv'\n",
    "data = loadDataset(paths,target)\n",
    "X = data.get('data')\n",
    "y = data.get('target')\n",
    "\n",
    "# remove 0 variance\n",
    "col_index = np.where(X.var()!=0)\n",
    "columns = X.loc[:, X.var()== 0.0].columns.values\n",
    "X = X.loc[:, X.var() != 0.0]\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = X.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper_traingle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop_cols = [column for column in upper_traingle.columns if any(upper_traingle[column] >= 0.80)]\n",
    "\n",
    "# Drop features \n",
    "X = X.drop(X[to_drop_cols], axis=1)\n",
    "print(\"current shape:\",X.shape)\n",
    "\n",
    "print(X['touchAccuracy_median'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimalModelSelection(model,param_grid,X,y,method='grid'):\n",
    "    '''Tune the hyperparameters to find the best score personality data'''\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "    from sklearn.pipeline import make_pipeline,Pipeline\n",
    "    from sklearn.model_selection import KFold,GridSearchCV,RandomizedSearchCV\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    scoring={'r2':'r2','mse':'neg_mean_squared_error','mae':'neg_mean_absolute_error'}\n",
    "    if(method=='grid'):\n",
    "        search = GridSearchCV(model, param_grid, cv=10,n_jobs=-1,scoring=scoring,return_train_score=True,refit='r2')\n",
    "        search.fit(X,y)\n",
    "    if(method=='random'):\n",
    "        search=RandomizedSearchCV(estimator = model, param_distributions = param_grid, \n",
    "                               n_iter = 100, cv = 10, verbose=1, \n",
    "                               random_state=32, n_jobs = -1,scoring=scoring,return_train_score=True,refit='r2')\n",
    "        search.fit(X,y)\n",
    "    \n",
    "    print('Best params: {}'.format(search.best_params_))\n",
    "    logging.info('Best params: {}'.format(search.best_params_))\n",
    "    print('RMSE: %0.2f'%(np.sqrt(-search.cv_results_['mean_test_mse'][search.best_index_])))\n",
    "    print(\"R2(Validation): %0.2f (+/- %0.2f)\" % (search.best_score_,search.cv_results_['std_test_r2'][search.best_index_]))\n",
    "    print(\"R2(Train): %0.2f (+/- %0.2f)\" % (search.cv_results_['mean_train_r2'][search.best_index_],\n",
    "                                                 search.cv_results_['std_train_r2'][search.best_index_]))\n",
    "    print(\"MAE(Validation): %0.2f (+/- %0.2f)\" % (-search.cv_results_['mean_test_mae'][search.best_index_],\n",
    "                                                  search.cv_results_['std_test_mae'][search.best_index_]))\n",
    "    print(\"MAE(Train): %0.2f (+/- %0.2f)\" % (-search.cv_results_['mean_train_mae'][search.best_index_],\n",
    "                                                 search.cv_results_['std_train_mae'][search.best_index_]))\n",
    "    \n",
    "    logging.info('RMSE: %0.2f'%(np.sqrt(-search.cv_results_['mean_test_mse'][search.best_index_])))\n",
    "    logging.info(\"R2: %0.2f (+/- %0.2f)\" % (search.best_score_,search.cv_results_['std_test_r2'][search.best_index_]))\n",
    "    return search.best_estimator_,search.best_params_, search.best_score_,search.cv_results_,search.best_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_evaluation(pathsArr,model,param_grid,method='grid',transformation=False,n_components=0.95):\n",
    "    \n",
    "    if transformation==False:\n",
    "        targets=['Neuroticism', 'Extraversion', \n",
    "                        'Openness', 'Agreeableness','Conscientiousness']\n",
    "    else:\n",
    "        '''For transformation'''\n",
    "        not_columns=['Neuroticism', 'Extraversion', \n",
    "                        'Openness', 'Agreeableness','Conscientiousness']\n",
    "        normality_test_features_path='/mnt/vdb1/Personality-Ratings/NormalityCheck/combined_univariate_normality_test_features_mahalanobis_transformed.csv'\n",
    "        mahalanobis = pd.read_csv(normality_test_features_path)\n",
    "        mahalanobis = list(mahalanobis[mahalanobis['Normality']==True]['Features'].values)\n",
    "\n",
    "        for col in not_columns:\n",
    "            if(col in mahalanobis):\n",
    "                mahalanobis.remove(col)\n",
    "        targets=['Neuroticism', 'Extraversion', \n",
    "                        'Openness', 'Agreeableness','Conscientiousness']\n",
    "    \n",
    "    #store the results\n",
    "    results_r2_val_scores={}\n",
    "    results_r2_test_scores={}\n",
    "    results_r2_train_scores={}\n",
    "    results_rmse_test={}\n",
    "    results_rmse_train={}\n",
    "    results_rmse_val={}\n",
    "    results_adjusted_r2_val_scores={}\n",
    "    results_std_r2_val_scores={}\n",
    "    results={}\n",
    "    results_val_mae={}\n",
    "    results_test_mae={}\n",
    "    results_train_mae={}\n",
    "    predictions={}\n",
    "    results_val_mape={}\n",
    "    results_test_mape={}\n",
    "    i=0\n",
    "    for target in targets:\n",
    "        logging.info('Prediction for {}'.format(target))\n",
    "        print('Prediction for {}'.format(target))\n",
    "        personality=loadDataset(paths=pathsArr,target=target)\n",
    "        X=personality.get('data')\n",
    "#             print(X.isnull().values.any())\n",
    "        columns = X.loc[:, X.var() == 0.0].columns\n",
    "        print(\"columns thrown away because they have 0 variance:\",columns)\n",
    "        X = X.loc[:, X.var() != 0.0]\n",
    "        \n",
    "        print(\"Shape of the data cleaning data :\",X.shape)\n",
    "        if transformation==True:\n",
    "            X=X[mahalanobis]\n",
    "            print(\"Shape of the data after selected transformed columns:\",X.shape)\n",
    "#                 i=i+1\n",
    "        y=personality.get('target')\n",
    "    \n",
    "        # Create correlation matrix\n",
    "        corr_matrix = X.corr().abs()\n",
    "\n",
    "        # Select upper triangle of correlation matrix\n",
    "        upper_traingle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "        # Find index of feature columns with correlation greater than or equal to 0.80\n",
    "        to_drop_cols = [column for column in upper_traingle.columns if any(upper_traingle[column] >= 0.80)]\n",
    "        X=X.drop(X[to_drop_cols],axis=1)\n",
    "        \n",
    "        print(\"Current Shape of data:\",X.shape)\n",
    "    \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X)\n",
    "        X=scaler.transform(X)\n",
    "        print(np.where(np.isnan(X)==True))\n",
    "        print('number of principal components:',n_components)\n",
    "        pca= PCA(n_components=n_components)\n",
    "        pca.fit(X)\n",
    "        print('number of principal components:',pca.n_components_)\n",
    "            \n",
    "        logging.info('number of principal components: {}'.format(pca.n_components_))\n",
    "        predictors=pca.n_components_\n",
    "        X=pca.transform(X)\n",
    "                \n",
    "        estimator, best_params_,best_score_,cv_results_,best_index_ = optimalModelSelection(\n",
    "                    model,param_grid,X,y,method)\n",
    "\n",
    "        y_pred_train  = estimator.fit(X,y).predict(X)\n",
    "        \n",
    "        # calculate MAPE\n",
    "        mape_score_val = np.mean(np.abs((y - y_pred_train) / y)) * 100\n",
    "\n",
    "        '''Store the residuals in the table'''\n",
    "        residuals = np.array(y)- y_pred_train\n",
    "\n",
    "        #store it in a seperate table\n",
    "        prediction= {'Original':np.array(y),'Predicted':y_pred_train,'Residuals':residuals}\n",
    "        predictions[target]=prediction\n",
    "            \n",
    "        # RSquared score\n",
    "        results_r2_val_scores[target]=best_score_\n",
    "        results_r2_train_scores[target]=cv_results_['mean_train_r2'][best_index_]\n",
    "            \n",
    "        # RMSE \n",
    "        results_rmse_train[target]=np.sqrt(np.abs(cv_results_['mean_train_mse'][best_index_]))\n",
    "        results_rmse_val[target]= np.sqrt(np.abs(cv_results_['mean_test_mse'][best_index_]))\n",
    "        \n",
    "        results_std_r2_val_scores[target]= cv_results_['std_test_r2'][best_index_]\n",
    "            \n",
    "        # Adjusted RSquared\n",
    "        results_adjusted_r2_val_scores[target]=1 - (1-best_score_)*(len(y)-1)/(len(y)-predictors-1)\n",
    "            \n",
    "        # MAE \n",
    "        results_train_mae[target]= -cv_results_['mean_train_mae'][best_index_]\n",
    "        results_val_mae[target]= -cv_results_['mean_test_mae'][best_index_]\n",
    "        \n",
    "        #MAPE\n",
    "        results_val_mape[target]=mape_score_val\n",
    "            \n",
    "        print('*'*100)\n",
    "        logging.info('*'*100)\n",
    "                \n",
    "        \n",
    "    results['r2_train']=results_r2_train_scores\n",
    "    results['r2_validation']=results_r2_val_scores\n",
    "    results['rmse_train']=results_rmse_train\n",
    "    results['rmse_validation']=results_rmse_val\n",
    "    results['std_validation']= results_std_r2_val_scores\n",
    "    results['adjusted_r2_val']=results_adjusted_r2_val_scores\n",
    "    results['mae_train']=results_train_mae\n",
    "    results['mae_validation']=results_val_mae\n",
    "    results['mape_validation']=results_val_mape\n",
    "        \n",
    "    print('*'*100)\n",
    "    logging.info('*'*100)\n",
    "    return results, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAllModels(pathsArr, filename,transformation=False,n_components=0.95):\n",
    "    # #create models\n",
    "    \n",
    "    np.random.seed(32)\n",
    "    #linear regression\n",
    "    logging.info('********Applying Linear Regression****************')\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    fit_intercept_space=[True]\n",
    "    param_grid={'fit_intercept':fit_intercept_space}\n",
    "    regr= LinearRegression()\n",
    "    results_regr,predictions_regr= perform_evaluation(pathsArr,regr,param_grid,method='grid',transformation=transformation,n_components=n_components)\n",
    "\n",
    "    ## lasso regression\n",
    "    logging.info('********Applying Lasso Regression****************')\n",
    "    from sklearn.linear_model import Lasso\n",
    "    alpha_space = np.logspace(-2, 2, 100)\n",
    "    param_grid={'alpha':alpha_space}\n",
    "    lasso = Lasso(random_state=32)\n",
    "    results_lasso,predictions_lasso = perform_evaluation(pathsArr,lasso,param_grid,method='random',\n",
    "                                                        transformation=transformation,n_components=n_components)\n",
    "    \n",
    "\n",
    "\n",
    "    ## elastic net \n",
    "    logging.info('********Applying Elastic Net Regression****************')\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    alpha_space = np.logspace(-2, 2 , 50)\n",
    "    param_grid={'alpha':alpha_space}\n",
    "    enet = ElasticNet(random_state=32)\n",
    "    results_enet,predictions_enet = perform_evaluation(pathsArr,enet,param_grid,method='random',transformation=transformation,n_components=n_components)\n",
    "\n",
    "    ##create models\n",
    "    np.random.seed(19)\n",
    "    # support vector machines\n",
    "    logging.info('********Applying Support vector machine****************')\n",
    "    from sklearn.svm import SVR\n",
    "    C_space=np.logspace(-1,1,10)\n",
    "    epsilon_space= np.logspace(-3,1,10)\n",
    "    gamma_space = np.logspace(-3, -2, 10)\n",
    "    param_grid={'C':C_space,'epsilon':epsilon_space,'gamma':gamma_space}\n",
    "    svr = SVR(kernel = 'rbf')\n",
    "    results_svm,predictions_svm = perform_evaluation(pathsArr,svr,param_grid,method='random',transformation=transformation,n_components=n_components)\n",
    "\n",
    "\n",
    "     # random forest\n",
    "    logging.info('********Applying Random Forest****************')\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n",
    "    max_depth = [int(x) for x in np.linspace(1, 5, num = 5)]\n",
    "\n",
    "    min_samples_split = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "    min_samples_leaf = [int(x) for x in np.linspace(10, 60, num = 20)]\n",
    "    bootstrap = [True, False]\n",
    "    max_features=['auto','sqrt']\n",
    "    param_grid={'n_estimators': n_estimators,\n",
    "                    'max_depth': max_depth,\n",
    "                    'min_samples_split': min_samples_split,\n",
    "                    'min_samples_leaf': min_samples_leaf,\n",
    "                    'max_features':max_features\n",
    "                   }   \n",
    "    rf = RandomForestRegressor(random_state=32)\n",
    "    results_rf,predictions_rf= perform_evaluation(pathsArr,rf,param_grid,method='random',transformation=transformation,n_components=n_components)\n",
    "\n",
    "    #Linear regression using stochastic gradient descent\n",
    "    logging.info('********Applying Linear regression with stochastic gradient descent****************')\n",
    "    from sklearn.linear_model import SGDRegressor\n",
    "    param_grid={#'max_iter':[100,500,1000],\n",
    "                'max_iter':[50,100],\n",
    "                'penalty':[None],\n",
    "                'eta0':[0.01,0.1,0.5]\n",
    "                   }\n",
    "    sgd_reg = SGDRegressor(random_state=32)\n",
    "    results_sgd,predictions_sgd = perform_evaluation(pathsArr,sgd_reg,param_grid,transformation=transformation,n_components=n_components)\n",
    "\n",
    "# #     # MARS\n",
    "    np.random.seed(20)\n",
    "    logging.info('********Applying MARS****************')\n",
    "    from pyearth import Earth\n",
    "    max_degree_space=[1]\n",
    "    penalty_space=np.logspace(-1,1,20)\n",
    "    minspan_alpha=np.logspace(-3,1,20)\n",
    "    max_terms=[10,20,25]\n",
    "    endspan_alpha = [0.05]\n",
    "    param_grid={'max_degree':max_degree_space,\n",
    "        'penalty':penalty_space,\n",
    "                'use_fast':[True],\n",
    "        'max_terms':max_terms\n",
    "               }\n",
    "    mars= Earth()\n",
    "    results_mars,predictions_mars= perform_evaluation(pathsArr,mars,param_grid,method='grid',transformation=transformation,n_components=n_components)\n",
    "    \n",
    "    def createTable(results,name):\n",
    "        '''Creates the final table'''\n",
    "        df= pd.concat([\n",
    "        pd.DataFrame.from_dict(results.get('r2_train'),orient='index'),\n",
    "                  pd.DataFrame.from_dict(results.get('r2_validation'),orient='index'),\n",
    "                  pd.DataFrame.from_dict(results.get('rmse_train'),orient='index'),\n",
    "        pd.DataFrame.from_dict(results.get('rmse_validation'),orient='index'),\n",
    "                  pd.DataFrame.from_dict(results.get('std_validation'),orient='index'),\n",
    "                     pd.DataFrame.from_dict(results.get('adjusted_r2_val'),orient='index'),\n",
    "                       pd.DataFrame.from_dict(results.get('mae_train'),orient='index'),\n",
    "                      pd.DataFrame.from_dict(results.get('mae_validation'),orient='index'),\n",
    "        pd.DataFrame.from_dict(results.get('mape_validation'),orient='index')\n",
    "        ],axis=1)\n",
    "        \n",
    "        df.columns=[\n",
    "            #'R2(Test)',\n",
    "            'R2(Train)','R2(Validation)',\n",
    "                    'RMSE(Train)','RMSE(Validation)',\n",
    "                    'StandardError(Validation)',\n",
    "                    'Adjusted R2(Validation)',\n",
    "                    'MAE(Train)','MAE(Validation)',\n",
    "                     'MAPE(Validation)'\n",
    "                   ]\n",
    "        df['Target']=df.index\n",
    "        df['Algorithm']=name\n",
    "        df= df.reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    df_sgd=createTable(results_sgd,name='Linear Regression SGD')\n",
    "    df_lasso=createTable(results_lasso,name='Lasso Regression')\n",
    "    df_enet=createTable(results_enet,name='Elastic Net')\n",
    "    df_svm=createTable(results_svm,name='SVM')\n",
    "    df_rf=createTable(results_rf,name='Random Forest')\n",
    "    df_mars=createTable(results_mars,name='MARS')\n",
    "    df_lr=createTable(results_regr,name='Linear Regression')\n",
    "    \n",
    "    # result file saved here\n",
    "    pd.concat([\n",
    "        df_rf,df_svm,df_sgd,\n",
    "        df_lr,\n",
    "        df_lasso,\n",
    "        df_enet,df_mars\n",
    "    ]).to_csv(filename)\n",
    "    \n",
    "    del df_sgd,df_lasso,df_enet,df_svm,df_rf,df_mars\n",
    "    \n",
    "     \n",
    "    \n",
    "    def createPredictionsTable(predictions):\n",
    "        targets = ['Neuroticism','Extraversion', 'Openness', 'Agreeableness','Conscientiousness']\n",
    "        df = pd.DataFrame()\n",
    "        for target in targets:\n",
    "            pq= pd.DataFrame(predictions.get(target))\n",
    "            pq.rename(index=str, columns={\"Original\": \"Original_\"+target, \"Prediction\": \"Prediction_\"+target,\n",
    "                                          'Residuals':'Residuals_'+target}, inplace=True)\n",
    "            df = pd.concat([df,pq],axis=1)\n",
    "        return df\n",
    "    \n",
    "    df_sgd=createPredictionsTable(predictions_sgd)\n",
    "    df_lasso=createPredictionsTable(predictions_lasso)\n",
    "    df_enet=createPredictionsTable(predictions_enet)\n",
    "    df_svm=createPredictionsTable(predictions_svm)\n",
    "    df_rf=createPredictionsTable(predictions_rf)\n",
    "    df_mars=createPredictionsTable(predictions_mars)\n",
    "    df_lr=createPredictionsTable(predictions_regr)\n",
    "    \n",
    "    if transformation==False:\n",
    "        filename='PCA_alltargets_mahalanobis_PCA_'+str(n_components)+'_predictions.xlsx'\n",
    "    else:\n",
    "        filename='PCA_alltargets_mahalanobis_transformed_PCA_'+str(n_components)+'_predictions.xlsx'\n",
    "    \n",
    "    # residuals are saved here\n",
    "    with pd.ExcelWriter(filename) as writer:  # doctest: +SKIP\n",
    "        df_sgd.to_excel(writer, sheet_name='Linear Regression SGD')\n",
    "        df_lasso.to_excel(writer, sheet_name='Lasso Regression')\n",
    "        df_enet.to_excel(writer, sheet_name='Elastic Net')\n",
    "        df_svm.to_excel(writer, sheet_name='SVM')\n",
    "        df_rf.to_excel(writer, sheet_name='Random Forest')\n",
    "        df_mars.to_excel(writer, sheet_name='MARS')\n",
    "        df_lr.to_excel(writer, sheet_name='LR')\n",
    "    \n",
    "    print('File saved successfully')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study1+Study2 original\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on original distribution with 95% variance explained PC\n",
    "if __name__=='__main__':\n",
    "    paths=['/mnt/vdb1/datasets/files_generated/Personality/study1_features_data_out_mahalanobis.csv',\n",
    "                      '/mnt/vdb1/datasets/files_generated/Personality/study2_features_data_out_mahalanobis.csv']\n",
    "\n",
    "    filename='Tables/PCA_alltargets_mahalanobis_0.95PC.csv'\n",
    "    runAllModels(paths,filename,transformation=False,n_components=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on original distribution with 80% variance explained PC\n",
    "if __name__=='__main__':\n",
    "    paths=['/mnt/vdb1/datasets/files_generated/Personality/study1_features_data_out_mahalanobis.csv',\n",
    "                      '/mnt/vdb1/datasets/files_generated/Personality/study2_features_data_out_mahalanobis.csv']\n",
    "    filename='Tables/PCA_alltargets_mahalanobis_0.80PC.csv'\n",
    "    runAllModels(paths,filename,transformation=False,n_components=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on original distribution with 3 PC\n",
    "if __name__=='__main__':\n",
    "    paths=['/mnt/vdb1/datasets/files_generated/Personality/study1_features_data_out_mahalanobis.csv',\n",
    "                      '/mnt/vdb1/datasets/files_generated/Personality/study2_features_data_out_mahalanobis.csv']\n",
    "    filename='Tables/PCA_alltargets_mahalanobis_3PC.csv'\n",
    "    runAllModels(paths,filename,transformation=False,n_components=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study1+Study2 transformation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on transformed distribution with 95% variance explained PC\n",
    "if __name__=='__main__':\n",
    "    paths=['/mnt/vdb1/datasets/files_generated/Personality/combined_features_data_out_mahalanobis_transformedDistributions.csv']\n",
    "    filename='Tables/PCA_alltargets_mahalanobis_transformed_0.95PC.csv'\n",
    "    runAllModels(paths,filename,transformation=True,n_components=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on transformed distribution with 80% variance explained PC\n",
    "if __name__=='__main__':\n",
    "    paths=['/mnt/vdb1/datasets/files_generated/Personality/combined_features_data_out_mahalanobis_transformedDistributions.csv']\n",
    "    filename='Tables/PCA_alltargets_mahalanobis_transformed_0.80PC.csv'\n",
    "    runAllModels(paths,filename,transformation=True,n_components=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on transformed distribution with 3 PC\n",
    "if __name__=='__main__':\n",
    "    paths=['/mnt/vdb1/datasets/files_generated/Personality/combined_features_data_out_mahalanobis_transformedDistributions.csv']\n",
    "    filename='Tables/PCA_alltargets_mahalanobis_transformed_3PC.csv'\n",
    "    runAllModels(paths,filename,transformation=True,n_components=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
