{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to train ML algorithms after feature selection and feature extraction  except MARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as ns\n",
    "from sklearn.decomposition import PCA,FastICA,KernelPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from genetic_selection import GeneticSelectionCV\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from RegscorePy.aic import aic # for calculating Akaikeâ€™s Information Criterion\n",
    "from RegscorePy.bic import bic # for calculating Bayesian Information Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load MLOperationsUtilities.py\n",
    "def readDataFromCsv(file):\n",
    "    \"\"\" Read csv from files\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file: str\n",
    "        Filename to be read\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    pandas.DataFrame\n",
    "        Returns the dataframe containing the dataset\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    print (\"Reading the file from: \",file)\n",
    "    df = pd.read_csv(file)\n",
    "    return df\n",
    "\n",
    "def loadDataset(paths=['../datasets/files_generated/Personality/study1_features_data.csv',\n",
    "                      '../datasets/files_generated/Personality/study2_features_data.csv'],target='Neuroticism'):\n",
    "    \"\"\" prepares the data and loads it\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    paths: array\n",
    "        Filenames to be read\n",
    "    target: str\n",
    "        perosnality label to be specified\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    pandas.DataFrame\n",
    "        Returns the dataframe containing the dataset\n",
    "    \"\"\"\n",
    "    for path in paths:\n",
    "        if 'study1' in path:\n",
    "            df = readDataFromCsv(path)\n",
    "            df= df.select_dtypes (['int64','float64']).drop(['VP','age','user_id'],axis=1)\n",
    "            print('The shape of the data  currently in study1: ',df.shape)\n",
    "            X_study1,y_study1= df.drop(['Neuroticism', 'Extraversion', \n",
    "                                        'Openness', 'Agreeableness','Conscientiousness'],axis=1),df[target]\n",
    "        elif 'study2'in path:\n",
    "            df = readDataFromCsv(path)\n",
    "            df = df.select_dtypes(['int64','float64']).drop(['user_id','UserId','VP','Age','Handedness_Score'],axis=1)\n",
    "            print('The shape of the data  currently in study2: ',df.shape)\n",
    "            X_study2,y_study2=df.drop(['Neuroticism', 'Extraversion', 'Openness', 'Agreeableness','Conscientiousness'],axis=1),df[target]\n",
    "        else:\n",
    "            df = pd.read_csv(path,index_col=0)\n",
    "            X,y=df.drop(['Neuroticism', 'Extraversion', \n",
    "                                        'Openness', 'Agreeableness','Conscientiousness','user_id'],axis=1),df[target]\n",
    "    # concat both the studies\n",
    "    if(len(paths)>1):\n",
    "        X = pd.concat([X_study1,X_study2])\n",
    "        y= pd.concat([y_study1,y_study2])\n",
    "    \n",
    "    print('The shape of the data after concating both the studies {}'.format(X.shape))\n",
    "    print('The shape of the target after concating both the studies {}'.format(y.shape))\n",
    "    assert df.isnull().values.any()==False, 'Please check for null values'\n",
    "    df_result={'data':X,'target':y}\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimalModelSelection(model,param_grid,X,y,method='grid'):\n",
    "    '''Tune the hyperparameters to find the best score personality data'''\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "    from sklearn.pipeline import make_pipeline,Pipeline\n",
    "    from sklearn.model_selection import KFold,GridSearchCV,RandomizedSearchCV\n",
    "    \n",
    "    K = 10\n",
    "    kf = KFold(n_splits=K, shuffle=True,random_state=32)\n",
    "    \n",
    "    scoring={'r2':'r2','mse':'neg_mean_squared_error','mae':'neg_mean_absolute_error'}\n",
    "    if(method=='grid'):\n",
    "        search = GridSearchCV(model, param_grid, cv=kf,n_jobs=-1,scoring=scoring,return_train_score=True,refit='r2')\n",
    "        search.fit(X,y)\n",
    "    if(method=='random'):\n",
    "        search=RandomizedSearchCV(estimator = model, param_distributions = param_grid, \n",
    "                               n_iter = 100, cv = kf, verbose=1, \n",
    "                               random_state=32, n_jobs = -1,scoring=scoring,return_train_score=True,refit='r2')\n",
    "        search.fit(X,y)\n",
    "    \n",
    "    print('Best params: {}'.format(search.best_params_))\n",
    "    print('RMSE: %0.2f'%(np.sqrt(-search.cv_results_['mean_test_mse'][search.best_index_])))\n",
    "    print(\"R2(Validation): %0.2f (+/- %0.2f)\" % (search.best_score_,search.cv_results_['std_test_r2'][search.best_index_]))\n",
    "    print(\"R2(Train): %0.2f (+/- %0.2f)\" % (search.cv_results_['mean_train_r2'][search.best_index_],\n",
    "                                                 search.cv_results_['std_train_r2'][search.best_index_]))\n",
    "    print(\"MAE(Validation): %0.2f (+/- %0.2f)\" % (-search.cv_results_['mean_test_mae'][search.best_index_],\n",
    "                                                  search.cv_results_['std_test_mae'][search.best_index_]))\n",
    "    print(\"MAE(Train): %0.2f (+/- %0.2f)\" % (-search.cv_results_['mean_train_mae'][search.best_index_],\n",
    "                                                 search.cv_results_['std_train_mae'][search.best_index_]))\n",
    "     \n",
    "    return search.best_estimator_,search.best_params_, search.best_score_,search.cv_results_,search.best_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genetic_selection(estimator,X,y):\n",
    "    '''\n",
    "    Returns the selected columns after GA\n",
    "    '''\n",
    "    np.random.seed(10)\n",
    "    from pyearth import Earth\n",
    "    # calculate the optimal population size\n",
    "    if (isinstance(estimator,Earth)==False):\n",
    "#         population_size=math.ceil((267.43*np.log(X.shape[0]))-293.21) # reference from paper\n",
    "        population_size=50\n",
    "        generations=20\n",
    "    else:\n",
    "        population_size=100 \n",
    "        generations=40 # this may not lead to optimal solution and may suffer from premature convergence\n",
    "    selector = GeneticSelectionCV(estimator,\n",
    "                                          cv=5,\n",
    "                                          scoring=\"r2\",\n",
    "                                          n_population=population_size,\n",
    "                                          crossover_proba=0.5,\n",
    "                                          mutation_proba=0.01,\n",
    "                                          n_generations=generations,\n",
    "                                          tournament_size=3,\n",
    "                                          caching=True,\n",
    "                                          n_jobs=-1)\n",
    "    start = time.time()\n",
    "    selector = selector.fit(X, y)\n",
    "    print(\"---Finished in %s seconds ---\" % (np.round(time.time() - start,3)))\n",
    "    print(\"Number of columns selected:\",len(np.where(selector.support_==True)[0]))\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        columns=X.columns[selector.support_] # returns the column names\n",
    "    else:\n",
    "        columns=np.where(selector.support_==True) # return the indices of the numpy array\n",
    "        \n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_evaluation(path, estimator,param_grid,method='grid',transformation=False):\n",
    "    if transformation==False:\n",
    "        targets=['Neuroticism', 'Extraversion', \n",
    "                        'Openness', 'Agreeableness','Conscientiousness']\n",
    "    else:\n",
    "        '''For transformation'''\n",
    "        not_columns=['Neuroticism', 'Extraversion', \n",
    "                        'Openness', 'Agreeableness','Conscientiousness']\n",
    "        normality_test_features_path='/mnt/vdb1/Personality-Ratings/NormalityCheck/combined_univariate_normality_test_features_mahalanobis_transformed.csv'\n",
    "#         normality_test_features_path='Tables/NormalityCheck/combined_univariate_normality_test_features_mahalanobis_transformed.csv'\n",
    "        mahalanobis = pd.read_csv(normality_test_features_path)\n",
    "        mahalanobis = list(mahalanobis[mahalanobis['Normality']==True]['Features'].values)\n",
    "\n",
    "        for col in not_columns:\n",
    "            if(col in mahalanobis):\n",
    "                mahalanobis.remove(col)\n",
    "        targets=['Neuroticism', 'Extraversion', \n",
    "                        'Openness', 'Agreeableness','Conscientiousness']\n",
    "        print(\"Columns that should not be selected are:\",mahalanobis)\n",
    "\n",
    "    results={}\n",
    "    predictions={}\n",
    "    for target in targets:\n",
    "        target_result={}\n",
    "        print('Performing prediction for target:',target)\n",
    "        \n",
    "        personality=loadDataset(paths=path,target=target)\n",
    "        X=personality.get('data')\n",
    "        y=personality.get('target')\n",
    "        columns = X.loc[:, X.var() == 0.0].columns\n",
    "        print(\"columns thrown away because they have 0 variance:\",columns)\n",
    "        X = X.loc[:, X.var() != 0.0]\n",
    "        print(\"Shape of the data after removing 0 variance columns:\",X.shape)\n",
    "        if transformation==True:\n",
    "            # apply only the normal columns\n",
    "            X=X[mahalanobis]\n",
    "            print(\"Shape of the data after selected transformed columns:\",X.shape)\n",
    "        \n",
    "        # Create correlation matrix\n",
    "        corr_matrix = X.corr().abs()\n",
    "\n",
    "        # Select upper triangle of correlation matrix\n",
    "        upper_traingle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "        # Find index of feature columns with correlation greater than or equal to 0.80\n",
    "        to_drop_cols = [column for column in upper_traingle.columns if any(upper_traingle[column] >= 0.80)]\n",
    "        X=X.drop(X[to_drop_cols],axis=1)\n",
    "        \n",
    "        print(\"Current Shape of data:\",X.shape)\n",
    "        \n",
    "        # features selected\n",
    "        selected_features= genetic_selection(estimator,X,y)\n",
    "            \n",
    "        # scale the data\n",
    "        scaler=StandardScaler()\n",
    "        X=scaler.fit_transform(X[selected_features])\n",
    "            \n",
    "        # tune hyperparameters on the optimal subset\n",
    "        best_estimator_,best_params_, best_score_,cv_results_,best_index_= optimalModelSelection(estimator,param_grid,X,y,method=method)\n",
    "        \n",
    "        # calculate the AIC\n",
    "        y_pred_train  = best_estimator_.fit(X,y).predict(X)\n",
    "\n",
    "        # calculate MAPE\n",
    "        mape_score_val = np.mean(np.abs((y - y_pred_train) / y)) * 100\n",
    "        \n",
    "\n",
    "        ''''Store the residuals in the table'''\n",
    "        residuals = np.array(y)- y_pred_train\n",
    "        #store it in a seperate table\n",
    "        prediction= {'Original':np.array(y),'Predicted':y_pred_train,'Residuals':residuals}\n",
    "\n",
    "        predictions[target]=prediction\n",
    "            \n",
    "        # append the results\n",
    "        target_result['R2(Validation)']=best_score_\n",
    "        target_result['Adjusted R2(Validation)']=1-(1-best_score_)*(len(y)-1)/(len(y)-len(selected_features)-1)\n",
    "        target_result['StandardError(Validation)']=cv_results_['std_test_r2'][best_index_]\n",
    "        target_result['RMSE(Validation)']=np.sqrt(np.abs(cv_results_['mean_test_mse'][best_index_]))\n",
    "        target_result['R2(Train)']=cv_results_['mean_train_r2'][best_index_]\n",
    "        target_result['RMSE(Train)']=np.sqrt(np.abs(cv_results_['mean_train_mse'][best_index_]))\n",
    "\n",
    "        target_result['#Features']=len(selected_features)\n",
    "        target_result['Features']=selected_features.values\n",
    "        target_result['MAE(Validation)']= -cv_results_['mean_test_mae'][best_index_]\n",
    "        target_result['MAE(Train)']= -cv_results_['mean_train_mae'][best_index_]\n",
    "        target_result['MAPE(Validation)']=mape_score_val\n",
    "\n",
    "        # store the result with respect to target\n",
    "        results[target]=target_result\n",
    "    \n",
    "    return results,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_evaluation_with_pca(path, estimator,param_grid,n_components,method='grid',transformation=False):\n",
    "    if transformation==False:\n",
    "        targets=['Neuroticism', 'Extraversion', \n",
    "                        'Openness', 'Agreeableness','Conscientiousness']\n",
    "    else:\n",
    "        '''For transformation'''\n",
    "        not_columns=['Neuroticism', 'Extraversion', \n",
    "                        'Openness', 'Agreeableness','Conscientiousness']\n",
    "        normality_test_features_path='/mnt/vdb1/Personality-Ratings/NormalityCheck/combined_univariate_normality_test_features_mahalanobis_transformed.csv'\n",
    "        mahalanobis = pd.read_csv(normality_test_features_path)\n",
    "        mahalanobis = list(mahalanobis[mahalanobis['Normality']==True]['Features'].values)\n",
    "\n",
    "        for col in not_columns:\n",
    "            if(col in mahalanobis):\n",
    "                mahalanobis.remove(col)\n",
    "        targets=['Neuroticism', 'Extraversion', \n",
    "                        'Openness', 'Agreeableness','Conscientiousness']\n",
    "        print(\"Columns that should not be selected are:\",mahalanobis)\n",
    "\n",
    "    \n",
    "    results={}\n",
    "    predictions={}\n",
    "    for target in targets:\n",
    "        print(target)\n",
    "        target_result={}\n",
    "        print('Performing prediction for target:',target)\n",
    "        \n",
    "        personality=loadDataset(paths=path,target=target)\n",
    "        X=personality.get('data')\n",
    "        y=personality.get('target')\n",
    "        columns = X.loc[:, X.var() == 0.0].columns\n",
    "        print(\"columns thrown away because they have 0 variance:\",columns)\n",
    "        X = X.loc[:, X.var() != 0.0]\n",
    "        print(\"Shape of the data after removing 0 variance columns:\",X.shape)\n",
    "        if transformation==True:\n",
    "            # apply only the normal columns\n",
    "            X=X[mahalanobis]\n",
    "            print(\"Shape of the data after selected transformed columns:\",X.shape)\n",
    "        \n",
    "        # Create correlation matrix\n",
    "        corr_matrix = X.corr().abs()\n",
    "\n",
    "        # Select upper triangle of correlation matrix\n",
    "        upper_traingle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "        # Find index of feature columns with correlation greater than or equal to 0.80\n",
    "        to_drop_cols = [column for column in upper_traingle.columns if any(upper_traingle[column] >= 0.80)]\n",
    "        X=X.drop(X[to_drop_cols],axis=1)\n",
    "        \n",
    "        print(\"Current Shape of data:\",X.shape)\n",
    "        \n",
    "            \n",
    "        # scale the data\n",
    "        scaler=StandardScaler()\n",
    "        X=scaler.fit_transform(X)\n",
    "        \n",
    "        # perform PCA\n",
    "        print('inside ga with pca function')\n",
    "        print(type(n_components))\n",
    "        print(n_components)\n",
    "        pca= PCA(n_components=n_components)\n",
    "        pca.fit(X)\n",
    "        X=pca.transform(X)\n",
    "        print('number of principal components:',pca.n_components_)\n",
    "        \n",
    "        # apply genetic algorithm to select the best PC\n",
    "        selected_features= genetic_selection(estimator,X,y)\n",
    "        \n",
    "            \n",
    "        # tune hyperparameters on the optimal subset\n",
    "        best_estimator_,best_params_, best_score_,cv_results_,best_index_= optimalModelSelection(estimator,\n",
    "                                                                                                 param_grid,\n",
    "                                                                                                 X[:,selected_features[0]],y,method=method)\n",
    "        \n",
    "        y_pred_train  = best_estimator_.fit(X[:,selected_features[0]],y).predict(X[:,selected_features[0]])\n",
    "        print(y_pred_train)\n",
    "        # calculate MAPE\n",
    "        mape_score_val = np.mean(np.abs((y - y_pred_train) / y)) * 100\n",
    "       \n",
    "        ''''Store the residuals in the table'''\n",
    "        residuals = np.array(y)- y_pred_train\n",
    "        #store it in a seperate table\n",
    "        prediction= {'Original':np.array(y),'Predicted':y_pred_train,'Residuals':residuals}\n",
    "        predictions[target]=prediction\n",
    "            \n",
    "        # append the results\n",
    "        target_result['R2(Validation)']=best_score_\n",
    "        target_result['Adjusted R2(Validation)']=1-(1-best_score_)*(len(y)-1)/(len(y)-X[:,selected_features[0]].shape[1]-1)\n",
    "        target_result['StandardError(Validation)']=cv_results_['std_test_r2'][best_index_]\n",
    "        target_result['RMSE(Validation)']=np.sqrt(np.abs(cv_results_['mean_test_mse'][best_index_]))\n",
    "        target_result['R2(Train)']=cv_results_['mean_train_r2'][best_index_]\n",
    "        target_result['RMSE(Train)']=np.sqrt(np.abs(cv_results_['mean_train_mse'][best_index_]))\n",
    "        target_result['#Features']=len(selected_features[0])\n",
    "        target_result['Features']=selected_features[0]\n",
    "        target_result['MAE(Validation)']= -cv_results_['mean_test_mae'][best_index_]\n",
    "        target_result['MAE(Train)']= -cv_results_['mean_train_mae'][best_index_]\n",
    "        target_result['MAPE(Validation)']=mape_score_val\n",
    "        \n",
    "        # store the result with respect to target\n",
    "        results[target]=target_result\n",
    "    return results, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create models\n",
    "def runAllModels(path, filename,n_components=None,transformation=False,perform_pca=False):\n",
    "    \n",
    "    # random forest\n",
    "    print('********Applying Random forest****************')\n",
    "    rf = RandomForestRegressor(random_state=101)\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n",
    "    max_depth = [int(x) for x in np.linspace(1, 5, num = 5)]\n",
    "    min_samples_split = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "    min_samples_leaf = [int(x) for x in np.linspace(10, 60, num = 20)]\n",
    "    bootstrap = [True, False]\n",
    "    max_features=['auto','sqrt']\n",
    "    param_grid={'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_split': min_samples_split,\n",
    "                'min_samples_leaf': min_samples_leaf,\n",
    "#                 'bootstrap': bootstrap,\n",
    "                'max_features':max_features\n",
    "               }\n",
    "    if perform_pca==True:\n",
    "        results_rf,predictions_rf= perform_evaluation_with_pca(path,rf,param_grid,n_components,method='random',transformation=transformation)\n",
    "    else:\n",
    "        results_rf,predictions_rf= perform_evaluation(path,rf,param_grid,method='random',transformation=transformation)            \n",
    "    print(predictions_rf)\n",
    "    np.random.seed(15)\n",
    "    print('********Applying Support vector machine****************')\n",
    "    from sklearn.svm import SVR\n",
    "#     C_space=np.logspace(-1,0,10)\n",
    "    C_space=np.logspace(-1,1,10)\n",
    "#     epsilon_space= np.logspace(-1,0,10)\n",
    "    epsilon_space= np.logspace(-1,0,10)\n",
    "#     gamma_space = np.logspace(-3, 3, 10)\n",
    "    gamma_space = np.logspace(-3, -2, 10)\n",
    "    param_grid={'C':C_space,'epsilon':epsilon_space,'gamma':gamma_space}\n",
    "    svr = SVR(kernel = 'rbf')\n",
    "    if perform_pca==True:\n",
    "        results_svm,predictions_svm = perform_evaluation_with_pca(path,svr,param_grid,n_components,method='random',transformation=transformation)\n",
    "    else:\n",
    "        results_svm,predictions_svm = perform_evaluation(path,svr,param_grid,method='random',transformation=transformation)\n",
    "\n",
    "    print('********Applying Linear regression with stochastic gradient descent****************')\n",
    "    from sklearn.linear_model import SGDRegressor\n",
    "    param_grid={#'max_iter':[100,500,1000],\n",
    "                'max_iter':[50,100],\n",
    "                'penalty':[None],\n",
    "                'eta0':[0.01,0.1,0.5]\n",
    "               }\n",
    "    sgd_reg = SGDRegressor(random_state=32)\n",
    "    if perform_pca==True:\n",
    "        print(n_components)\n",
    "        print(type(n_components))\n",
    "        results_sgd,predictions_sgd = perform_evaluation_with_pca(path,sgd_reg,param_grid,n_components,transformation=transformation)\n",
    "    else:\n",
    "        results_sgd, predictions_sgd = perform_evaluation(path,sgd_reg,param_grid,transformation=transformation)\n",
    "    \n",
    "\n",
    "\n",
    "    ## lasso regression\n",
    "    print('********Applying Lasso Regression****************')\n",
    "    from sklearn.linear_model import Lasso\n",
    "#     alpha_space = np.logspace(-4, 0, 50)\n",
    "    alpha_space = np.logspace(0, 1, 100)\n",
    "    param_grid={'alpha':alpha_space}\n",
    "    lasso = Lasso(random_state=32)\n",
    "    if perform_pca==True:\n",
    "        results_lasso, predictions_lasso = perform_evaluation_with_pca(path,lasso,param_grid,n_components,transformation=transformation)\n",
    "    else:\n",
    "        results_lasso, predictions_lasso = perform_evaluation(path,lasso,param_grid,transformation=transformation)\n",
    "    \n",
    "\n",
    "    ## elastic net \n",
    "    print('********Applying Elastic Net Regression****************')\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "#     alpha_space = np.logspace(-4, 0, 50)\n",
    "    alpha_space = np.logspace(0, 2 , 50)\n",
    "    param_grid={'alpha':alpha_space}\n",
    "    enet = ElasticNet(random_state=32)\n",
    "    if perform_pca==True:\n",
    "        results_enet,predictions_enet = perform_evaluation_with_pca(path,enet,param_grid,n_components,transformation=transformation)\n",
    "    else:\n",
    "        results_enet, predictions_enet = perform_evaluation(path,enet,param_grid,transformation=transformation)\n",
    "    \n",
    "    \n",
    "    np.random.seed(32)\n",
    "    #MARS\n",
    "    print('********Applying MARS****************')\n",
    "    from pyearth import Earth\n",
    "    max_degree_space=[1]\n",
    "    penalty_space=np.logspace(-1,1,20)\n",
    "    minspan_alpha=np.logspace(-3,1,20)\n",
    "    max_terms=[10,20,25]\n",
    "    # endspan_alpha= np.linspace(0, 1.0, num = 10)\n",
    "    # endspan=[5]\n",
    "    param_grid={'max_degree':max_degree_space,\n",
    "        'penalty':penalty_space,\n",
    "                'use_fast':[True],\n",
    "        'max_terms':max_terms\n",
    "               }\n",
    "\n",
    "    df_rf=pd.DataFrame(results_rf).T\n",
    "    df_rf['Target']=df_rf.index\n",
    "    df_rf=df_rf.reset_index(drop=True)\n",
    "    df_rf['Algorithm']='Random Forest'\n",
    "    df_rf.set_index(['Algorithm'])\n",
    "\n",
    "    df_svm=pd.DataFrame(results_svm).T\n",
    "    df_svm['Target']=df_svm.index\n",
    "    df_svm=df_svm.reset_index(drop=True)\n",
    "    df_svm['Algorithm']='SVM'\n",
    "    df_svm.set_index(['Algorithm'])\n",
    "\n",
    "    df_sgd=pd.DataFrame(results_sgd).T\n",
    "    df_sgd['Target']=df_sgd.index\n",
    "    df_sgd=df_sgd.reset_index(drop=True)\n",
    "    df_sgd['Algorithm']='Linear regression'\n",
    "    df_sgd.set_index(['Algorithm'])\n",
    "\n",
    "    df_lasso=pd.DataFrame(results_lasso).T\n",
    "    df_lasso['Target']=df_lasso.index\n",
    "    df_lasso=df_lasso.reset_index(drop=True)\n",
    "    df_lasso['Algorithm']='Lasso Regression'\n",
    "    df_lasso.set_index(['Algorithm'])\n",
    "\n",
    "    df_enet=pd.DataFrame(results_enet).T\n",
    "    df_enet['Target']=df_enet.index\n",
    "    df_enet=df_enet.reset_index(drop=True)\n",
    "    df_enet['Algorithm']='Elastic Net'\n",
    "    df_enet.set_index(['Algorithm'])\n",
    "\n",
    "#     concat the df\n",
    "    pd.concat([\n",
    "        df_rf,df_svm,df_sgd,\n",
    "        df_lasso,\n",
    "        df_enet,\n",
    "    ]).to_csv(filename,index=False)\n",
    "    print(\"File saved\")\n",
    "    del df_rf,df_svm,df_sgd,df_lasso,df_enet,\n",
    "    \n",
    "    def createPredictionsTable(predictions):\n",
    "        targets = ['Neuroticism','Extraversion', 'Openness', 'Agreeableness','Conscientiousness']\n",
    "        df = pd.DataFrame()\n",
    "        for target in targets:\n",
    "            pq= pd.DataFrame(predictions.get(target))\n",
    "            pq.rename(index=str, columns={\"Original\": \"Original_\"+target, \"Prediction\": \"Prediction_\"+target,\n",
    "                                          'Residuals':'Residuals_'+target}, inplace=True)\n",
    "\n",
    "            df = pd.concat([df,pq],axis=1)\n",
    "        return df\n",
    "    \n",
    "    df_sgd=createPredictionsTable(predictions_sgd)\n",
    "    df_lasso=createPredictionsTable(predictions_lasso)\n",
    "    df_enet=createPredictionsTable(predictions_enet)\n",
    "    df_svm=createPredictionsTable(predictions_svm)\n",
    "    df_rf=createPredictionsTable(predictions_rf)\n",
    "    if transformation==False and perform_pca==True:\n",
    "        filename='feature_selection_alltargets_mahalanobis_PCA_'+str(n_components)+'_predictions.xlsx'\n",
    "    elif transformation==False and perform_pca==False:\n",
    "        filename='feature_selection_alltargets_mahalanobis'+'_predictions.xlsx'\n",
    "    elif transformation==True and perform_pca==True:\n",
    "        filename='feature_selection_alltargets_mahalanobis_transformed_PCA_'+str(n_components)+'_predictions.xlsx'\n",
    "    else:\n",
    "        filename='feature_selection_alltargets_mahalanobis_transformed'+'_predictions.xlsx'\n",
    "       \n",
    "    with pd.ExcelWriter(filename) as writer:  # doctest: +SKIP\n",
    "        df_sgd.to_excel(writer, sheet_name='Linear Regression')\n",
    "        df_lasso.to_excel(writer, sheet_name='Lasso Regression')\n",
    "        df_enet.to_excel(writer, sheet_name='Elastic Net')\n",
    "        df_svm.to_excel(writer, sheet_name='SVM')\n",
    "        df_rf.to_excel(writer, sheet_name='Random Forest')\n",
    "        print('File with filename %s saved successfully'%(filename))\n",
    "    \n",
    "    print('file saved sucessfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study1 + Study2 original\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on original distribution\n",
    "#/mnt/vdb1\n",
    "if __name__=='__main__':\n",
    "    paths=['/mnt/vdb1/datasets/files_generated/Personality/study1_features_data_out_mahalanobis.csv',\n",
    "                      '/mnt/vdb1/datasets/files_generated/Personality/study2_features_data_out_mahalanobis.csv']\n",
    "    filename='Tables/feature_selection_mahalanobis_alltargets.csv'\n",
    "    runAllModels(paths,filename,transformation=False,perform_pca=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study1  + Study2 Transformation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model data on transformed distribution features\n",
    "if __name__=='__main__':\n",
    "    path=['/mnt/vdb1/datasets/files_generated/Personality/combined_features_data_out_mahalanobis_transformedDistributions.csv']\n",
    "    filename='Tables/feature_selection_mahalanobis_transformed_alltargets.csv'\n",
    "    runAllModels(path,filename,transformation=True,perform_pca=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
