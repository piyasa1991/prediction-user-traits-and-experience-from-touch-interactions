{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to train ML algorithms after feature selection and feature extraction  except MARS for STUDY1 and STUDY2\n",
    "\n",
    "- MARS is performed using R caret. Check the file MARS_GA.R for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as ns\n",
    "from sklearn.decomposition import PCA,FastICA,KernelPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from genetic_selection import GeneticSelectionCV\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from RegscorePy.aic import aic # for calculating Akaikeâ€™s Information Criterion\n",
    "from RegscorePy.bic import bic # for calculating Bayesian Information Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDataFromCsv(file):\n",
    "    import pandas as pd\n",
    "    print (\"Reading the file from: \",file)\n",
    "    df = pd.read_csv(file)\n",
    "    return df\n",
    "\n",
    "def loadDataset(data='study1',path='../datasets/files_generated/UX/study1_features_data.csv',target='PQ',app='Spell'):    \n",
    "    df = readDataFromCsv(path)\n",
    "    df=df[df['App']==app]   \n",
    "    print('The shape of the data  currently: ',df.shape)\n",
    "    \n",
    "    ## This should not have been there\n",
    "    if(df.isnull().values.any()==True and data=='study1'):\n",
    "        df = df.dropna()\n",
    "        print('The shape of the data after dropping null values: ',df.shape)\n",
    "    if data == 'startData':\n",
    "        X,y= df.drop(['PQ', 'ATT', 'HQI', 'HQS', 'HQ'],axis=1),df[target]\n",
    "    elif data=='study1':\n",
    "        X,y= df.drop(['user_id','App','Cond','sessionNr','SEA', 'PQ', 'ATT', 'HQI', 'HQS', 'HQ'],axis=1),df[target]\n",
    "    elif data=='study2':\n",
    "        X,y=df.drop(['sessionNr','App','user_id','Size','UserId', 'Session', \n",
    "                     'PQ', 'ATT', 'HQI', 'HQS', 'HQ', 'IconSize'],axis=1),df[target]\n",
    "        print('inside study2 if')\n",
    "    df_result={'data':X,'target':y}\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def optimalModelSelection(model,param_grid,X,y,method='grid'):\n",
    "    \"\"\"Tune the hyperparameters to find the best score\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "    from sklearn.pipeline import make_pipeline,Pipeline\n",
    "    from sklearn.model_selection import KFold,GridSearchCV,RandomizedSearchCV\n",
    "    \n",
    "    K = 10\n",
    "    kf = KFold(n_splits=K, shuffle=True,random_state=32)\n",
    "    \n",
    "    scoring={'r2':'r2','mse':'neg_mean_squared_error','mae':'neg_mean_absolute_error'}\n",
    "    if(method=='grid'):\n",
    "        search = GridSearchCV(model, param_grid, cv=kf,n_jobs=-1,scoring=scoring,return_train_score=True,refit='r2')\n",
    "        search.fit(X,y)\n",
    "    if(method=='random'):\n",
    "        search=RandomizedSearchCV(estimator = model, param_distributions = param_grid, \n",
    "                               n_iter = 100, cv = kf, verbose=1, \n",
    "                               random_state=32, n_jobs = -1,scoring=scoring,return_train_score=True,refit='r2')\n",
    "        search.fit(X,y)\n",
    "\n",
    "    print('Best params: {}'.format(search.best_params_))\n",
    "    print('RMSE: %0.2f'%(np.sqrt(-search.cv_results_['mean_test_mse'][search.best_index_])))\n",
    "    print(\"R2(Validation): %0.2f (+/- %0.2f)\" % (search.best_score_,search.cv_results_['std_test_r2'][search.best_index_]))\n",
    "    print(\"R2(Train): %0.2f (+/- %0.2f)\" % (search.cv_results_['mean_train_r2'][search.best_index_],\n",
    "                                                 search.cv_results_['std_train_r2'][search.best_index_]))\n",
    "    print(\"MAE(Validation): %0.2f (+/- %0.2f)\" % (-search.cv_results_['mean_test_mae'][search.best_index_],\n",
    "                                                  search.cv_results_['std_test_mae'][search.best_index_]))\n",
    "    print(\"MAE(Train): %0.2f (+/- %0.2f)\" % (-search.cv_results_['mean_train_mae'][search.best_index_],\n",
    "                                                 search.cv_results_['std_train_mae'][search.best_index_]))\n",
    "    \n",
    "    return search.best_estimator_,search.best_params_, search.best_score_,search.cv_results_,search.best_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genetic_selection(estimator,X,y):\n",
    "    \"\"\" Returns the selected columns after GA\n",
    "    \"\"\"\n",
    "    np.random.seed(10)\n",
    "    from pyearth import Earth\n",
    "    # calculate the optimal population size\n",
    "    if (isinstance(estimator,Earth)==False):\n",
    "        population_size=10\n",
    "        generations=20\n",
    "    else:\n",
    "        population_size=100 \n",
    "        generations=40 # this may not lead to optimal solution and may suffer from premature convergence\n",
    "    selector = GeneticSelectionCV(estimator,\n",
    "                                          cv=5,\n",
    "                                          scoring=\"r2\",\n",
    "                                          n_population=population_size,\n",
    "                                          crossover_proba=0.5,\n",
    "                                          mutation_proba=0.01,\n",
    "                                          n_generations=generations,\n",
    "                                          tournament_size=3,\n",
    "                                          caching=True,\n",
    "                                          n_jobs=-1)\n",
    "    start = time.time()\n",
    "    selector = selector.fit(X, y)\n",
    "    print(\"---Finished in %s seconds ---\" % (np.round(time.time() - start,3)))\n",
    "    \n",
    "    print(\"Number of columns selected:\",len(np.where(selector.support_==True)[0]))\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        columns=X.columns[selector.support_] # returns the column names\n",
    "    else:\n",
    "        columns=np.where(selector.support_==True) # return the indices of the numpy array\n",
    "        \n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_evaluation(path, estimator,param_grid,method='grid',transformation=False, data='study1'):\n",
    "    \"\"\"Trains the algorithm and Feature Selection\n",
    "    \"\"\"\n",
    "    if transformation==True:\n",
    "        # consider only the normally distributed columns\n",
    "        not_columns=['SEA','PQ','ATT', 'HQI', 'HQ','HQS']\n",
    "        if data=='study1':\n",
    "             normality_test_features_path ='/mnt/vdb1/UX-Ratings/NormalityCheck/study1_univariate_normality_test_features_mahalanobis_transformed.csv'\n",
    "        else:\n",
    "            normality_test_features_path ='/mnt/vdb1/UX-Ratings/NormalityCheck/study2_univariate_normality_test_features_mahalanobis_transformed.csv'\n",
    "        mahalanobis = pd.read_csv(normality_test_features_path)\n",
    "        mahalanobis = list(mahalanobis[mahalanobis['Normality']==True]['Features'].values)\n",
    "        for col in not_columns:\n",
    "            if(col in mahalanobis):\n",
    "                mahalanobis.remove(col)\n",
    "        print(\"Columns that should not be selected are:\",mahalanobis)\n",
    "\n",
    "    targets=['PQ', 'ATT']\n",
    "    results={}\n",
    "    predictions={}\n",
    "    for target in targets:\n",
    "        target_result={}\n",
    "        print('Performing prediction for target:',target)\n",
    "        \n",
    "        personality=loadDataset(data=data,path=path,target=target)\n",
    "        X=personality.get('data')\n",
    "        columns = X.loc[:, X.var() == 0.0].columns\n",
    "        print(\"columns thrown away because they have 0 variance:\",columns)\n",
    "        X = X.loc[:, X.var() != 0.0]\n",
    "        y=personality.get('target')\n",
    "        \n",
    "        if transformation==True:\n",
    "            # apply only the normal columns\n",
    "            X=X[mahalanobis]\n",
    "            \n",
    "        # Create correlation matrix\n",
    "        corr_matrix = X.select_dtypes(['float64']).corr().abs()\n",
    "\n",
    "        # Select upper triangle of correlation matrix\n",
    "        upper_traingle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "        # Find index of feature columns with correlation greater than 0.95\n",
    "        to_drop_cols = [column for column in upper_traingle.columns if any(upper_traingle[column] >= 0.80)]\n",
    "            \n",
    "        # Drop features \n",
    "        X = X.drop(X[to_drop_cols], axis=1)\n",
    "            \n",
    "        print(\"Shape of the data after removing 0 variance highly correlated data:\",X.shape)\n",
    "        \n",
    "        # split the data into train test set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "        print(\"Shape of training data:\",X_train.shape)\n",
    "        print(\"Shape of test data:\",X_test.shape)\n",
    "            \n",
    "        # perform genetic algorithm\n",
    "        #if(isinstance(estimator,Earth)==False):\n",
    "        \n",
    "        # features selected\n",
    "        selected_features= genetic_selection(estimator,X_train,y_train)\n",
    "        #else:\n",
    "            #selected_features_index= genetic_selection(estimator,np.array(X_train),np.array(y_train))\n",
    "            #selected_features= X.columns[selected_features_index]\n",
    "            \n",
    "        # scale the data\n",
    "        scaler=StandardScaler()\n",
    "        X_train=scaler.fit_transform(X_train[selected_features])\n",
    "        X_test= scaler.fit_transform(X_test[selected_features])\n",
    "        \n",
    "            \n",
    "        # tune hyperparameters on the optimal subset\n",
    "        best_estimator_,best_params_, best_score_,cv_results_,best_index_= optimalModelSelection(estimator,param_grid,X_train,y_train,method=method)\n",
    "        \n",
    "        # calculate the AIC\n",
    "        y_pred_train  = best_estimator_.fit(X_train,y_train).predict(X_train)\n",
    "        aic_score_val = aic(y_train,y_pred_train,X_train.shape[1])\n",
    "        # calculate Bayesian Information Criterion\n",
    "        bic_score_val = bic(y_train,y_pred_train,X_train.shape[1])\n",
    "        # calculate MAPE\n",
    "        mape_score_val = np.mean(np.abs((y_train - y_pred_train) / y_train)) * 100\n",
    "        \n",
    "        # predict on unseen data\n",
    "        y_pred=best_estimator_.predict(X_test)\n",
    "        score=r2_score(y_test,y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "        mae = mean_absolute_error(y_test,y_pred)\n",
    "        aic_score_test = aic(y_test,y_pred,X_test.shape[1])\n",
    "        bic_score_test = bic(y_test,y_pred,X_test.shape[1])\n",
    "        mape_score_test = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "        \n",
    "        print('Peforming predictions on unseen data')\n",
    "        print('Performance(R2):%0.2f | RMSE:%0.2f | MAE:%0.2f '%(score,rmse,mae))\n",
    "        \n",
    "        '''TODO: Store the residuals in the table for the model'''\n",
    "        residuals_test = np.array(y_test)- y_pred\n",
    "        \n",
    "        #store it in a seperate table\n",
    "        residuals = np.array(y_train)- y_pred_train\n",
    "        prediction= {'Original':np.array(y_train),'Predicted':y_pred_train,'Residuals':residuals}\n",
    "        predictions[target]=prediction\n",
    "            \n",
    "        # append the results\n",
    "        target_result['R2(Validation)']=best_score_\n",
    "        target_result['Adjusted R2(Validation)']=1-(1-best_score_)*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1)\n",
    "        target_result['StandardError(Validation)']=cv_results_['std_test_r2'][best_index_]\n",
    "        target_result['RMSE(Validation)']=np.sqrt(np.abs(cv_results_['mean_test_mse'][best_index_]))\n",
    "        target_result['R2(Train)']=cv_results_['mean_train_r2'][best_index_]\n",
    "        target_result['RMSE(Train)']=np.sqrt(np.abs(cv_results_['mean_train_mse'][best_index_]))\n",
    "        target_result['R2(Test)']=score\n",
    "        target_result['RMSE(Test)']=rmse\n",
    "        target_result['#Features']=len(selected_features)\n",
    "        target_result['Features']=selected_features.values\n",
    "        target_result['MAE(Validation)']= -cv_results_['mean_test_mae'][best_index_]\n",
    "        target_result['MAE(Train)']= -cv_results_['mean_train_mae'][best_index_]\n",
    "        target_result['MAE(Test)']=mae\n",
    "        target_result['AIC(Validation)']=aic_score_val\n",
    "        target_result['AIC(Test)']=aic_score_test\n",
    "        target_result['BIC(Validation)']=bic_score_val\n",
    "        target_result['BIC(Test)']=bic_score_test\n",
    "        target_result['MAPE(Validation)']=mape_score_val\n",
    "        target_result['MAPE(Test)']=mape_score_test\n",
    "        \n",
    "            \n",
    "        # store the result with respect to target\n",
    "        results[target]=target_result\n",
    "    \n",
    "    return results,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_evaluation_with_pca(path, estimator,param_grid,n_components,method='grid',transformation=False, data='study1'):\n",
    "    \"\"\"Trains the algorithm after PCA and Feature Selection\n",
    "    \"\"\"\n",
    "    if transformation==True:\n",
    "        # consider only the normally distributed columns\n",
    "        not_columns=['SEA','PQ','ATT', 'HQI', 'HQ','HQS']\n",
    "        if data=='study1':\n",
    "             normality_test_features_path ='/mnt/vdb1/UX-Ratings/NormalityCheck/study1_univariate_normality_test_features_mahalanobis_transformed.csv'\n",
    "        else:\n",
    "            normality_test_features_path ='/mnt/vdb1/UX-Ratings/NormalityCheck/study2_univariate_normality_test_features_mahalanobis_transformed.csv'\n",
    "        mahalanobis = pd.read_csv(normality_test_features_path)\n",
    "        mahalanobis = list(mahalanobis[mahalanobis['Normality']==True]['Features'].values)\n",
    "        for col in not_columns:\n",
    "            if(col in mahalanobis):\n",
    "                mahalanobis.remove(col)\n",
    "        print(\"Columns that should not be selected are:\",mahalanobis)\n",
    "\n",
    "    targets=['PQ', 'ATT']\n",
    "    results={}\n",
    "    predictions={}\n",
    "    for target in targets:\n",
    "        target_result={}\n",
    "        print('Performing prediction for target:',target)\n",
    "        \n",
    "        personality=loadDataset(data=data,path=path,target=target)\n",
    "        X=personality.get('data')\n",
    "        columns = X.loc[:, X.var() == 0.0].columns\n",
    "        print(\"columns thrown away because they have 0 variance:\",columns)\n",
    "        X = X.loc[:, X.var() != 0.0]\n",
    "        y=personality.get('target')\n",
    "        \n",
    "        if transformation==True:\n",
    "            # apply only the normal columns\n",
    "            X=X[mahalanobis]\n",
    "        \n",
    "        # Create correlation matrix\n",
    "        corr_matrix = X.select_dtypes(['float64']).corr().abs()\n",
    "\n",
    "        # Select upper triangle of correlation matrix\n",
    "        upper_traingle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "        # Find index of feature columns with correlation greater than 0.95\n",
    "        to_drop_cols = [column for column in upper_traingle.columns if any(upper_traingle[column] >= 0.80)]\n",
    "            \n",
    "        # Drop features \n",
    "        X = X.drop(X[to_drop_cols], axis=1)\n",
    "            \n",
    "        print(\"Shape of the data after removing 0 variance highly correlated data:\",X.shape)\n",
    "        \n",
    "        # split the data into train test set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "        print(\"Shape of training data:\",X_train.shape)\n",
    "        print(\"Shape of test data:\",X_test.shape)\n",
    "        print(\"Shape of test data:\",y_train.shape)\n",
    "            \n",
    "        # scale the data\n",
    "        scaler=StandardScaler()\n",
    "        X_train=scaler.fit_transform(X_train)\n",
    "        X_test= scaler.fit_transform(X_test)\n",
    "        \n",
    "        # perform PCA\n",
    "        print('inside ga with pca function')\n",
    "        print(type(n_components))\n",
    "        print(n_components)\n",
    "        pca= PCA(n_components=n_components)\n",
    "        pca.fit(X_train)\n",
    "        X_train = pca.transform(X_train)\n",
    "        X_test = pca.transform(X_test)\n",
    "        print('number of principal components:',pca.n_components_)\n",
    "        \n",
    "        # apply genetic algorithm to select the best PC\n",
    "        selected_features= genetic_selection(estimator,X_train,y_train)\n",
    "        \n",
    "            \n",
    "        # tune hyperparameters on the optimal subset\n",
    "        best_estimator_,best_params_, best_score_,cv_results_,best_index_= optimalModelSelection(estimator,\n",
    "                                                                                                 param_grid,\n",
    "                                                                                                 X_train[:,selected_features[0]],y_train,method=method)\n",
    "        \n",
    "        \n",
    "        y_pred_train  = best_estimator_.fit(X_train[:,selected_features[0]],y_train).predict(X_train[:,selected_features[0]])\n",
    "        \n",
    "        # calculate the AIC\n",
    "        aic_score_val = aic(y_train,y_pred_train,X_train[:,selected_features[0]].shape[1])\n",
    "        # calculate Bayesian Information Criterion\n",
    "        bic_score_val = bic(y_train,y_pred_train,X_train[:,selected_features[0]].shape[1])\n",
    "        # calculate MAPE\n",
    "        mape_score_val = np.mean(np.abs((y_train - y_pred_train) / y_train)) * 100\n",
    "        \n",
    "        # predict on unseen data\n",
    "        y_pred=best_estimator_.predict(X_test[:,selected_features[0]])\n",
    "        score=r2_score(y_test,y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "        mae = mean_absolute_error(y_test,y_pred)\n",
    "        \n",
    "        aic_score_test = aic(y_test,y_pred,X_test[:,selected_features[0]].shape[1])\n",
    "        bic_score_test = bic(y_test,y_pred,X_test[:,selected_features[0]].shape[1])\n",
    "        mape_score_test = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "        \n",
    "        print('Peforming predictions on unseen data')\n",
    "        print('Performance(R2):%0.2f | RMSE:%0.2f | MAE:%0.2f | MAPE:%0.2f'%(score,rmse, mae,mape_score_test))\n",
    "        \n",
    "        ''''TODO: Store the residuals in the table'''\n",
    "        residuals = np.array(y_test)- y_pred\n",
    "        print(len(residuals))\n",
    "        #store it in a seperate table\n",
    "        prediction= {'Original':np.array(y_test),'Predicted':y_pred,'Residuals':residuals}\n",
    "        predictions[target]=prediction\n",
    "            \n",
    "        # append the results\n",
    "        target_result['R2(Validation)']=best_score_\n",
    "        target_result['Adjusted R2(Validation)']=1-(1-best_score_)*(len(y_train)-1)/(len(y_train)-X_train[:,selected_features[0]].shape[1]-1)\n",
    "        target_result['StandardError(Validation)']=cv_results_['std_test_r2'][best_index_]\n",
    "        target_result['RMSE(Validation)']=np.sqrt(np.abs(cv_results_['mean_test_mse'][best_index_]))\n",
    "        target_result['R2(Train)']=cv_results_['mean_train_r2'][best_index_]\n",
    "        target_result['RMSE(Train)']=np.sqrt(np.abs(cv_results_['mean_train_mse'][best_index_]))\n",
    "        target_result['R2(Test)']=score\n",
    "        target_result['RMSE(Test)']=rmse\n",
    "        target_result['#Features']=len(selected_features[0])\n",
    "        target_result['Features']=selected_features[0]\n",
    "        target_result['MAE(Validation)']= -cv_results_['mean_test_mae'][best_index_]\n",
    "        target_result['MAE(Train)']= -cv_results_['mean_train_mae'][best_index_]\n",
    "        target_result['MAE(Test)']=mae\n",
    "        target_result['AIC(Validation)']=aic_score_val\n",
    "        target_result['AIC(Test)']=aic_score_test\n",
    "        target_result['BIC(Validation)']=bic_score_val\n",
    "        target_result['BIC(Test)']=bic_score_test\n",
    "        target_result['MAPE(Validation)']=mape_score_val\n",
    "        target_result['MAPE(Test)']=mape_score_test\n",
    "        \n",
    "        # store the result with respect to target\n",
    "        results[target]=target_result\n",
    "    \n",
    "    return results, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create models\n",
    "def runAllModels(path, filename,n_components=None,transformation=False,data='study1',perform_pca=False):\n",
    "    \n",
    "    # random forest\n",
    "    print('********Applying Random forest****************')\n",
    "    rf = RandomForestRegressor(random_state=101)\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n",
    "    max_depth = [int(x) for x in np.linspace(1, 5, num = 5)]\n",
    "    min_samples_split = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "    min_samples_leaf = [int(x) for x in np.linspace(10, 60, num = 20)]\n",
    "    bootstrap = [True, False]\n",
    "    max_features=['auto','sqrt']\n",
    "    param_grid={'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_split': min_samples_split,\n",
    "                'min_samples_leaf': min_samples_leaf,\n",
    "                'max_features':max_features\n",
    "               }\n",
    "    if perform_pca==True:\n",
    "        results_rf,predictions_rf= perform_evaluation_with_pca(path,rf,param_grid,n_components,method='random',transformation=transformation,data=data)\n",
    "    else:\n",
    "        results_rf,predictions_rf= perform_evaluation(path,rf,param_grid,method='random',transformation=transformation,data=data)            \n",
    "    \n",
    "    np.random.seed(15)\n",
    "    print('********Applying Support vector machine****************')\n",
    "    from sklearn.svm import SVR\n",
    "    C_space=np.logspace(-1,1,10)\n",
    "    epsilon_space= np.logspace(-1,0,10)\n",
    "    gamma_space = np.logspace(-3, -2, 10)\n",
    "    param_grid={'C':C_space,'epsilon':epsilon_space,'gamma':gamma_space}\n",
    "    svr = SVR(kernel = 'rbf')\n",
    "    if perform_pca==True:\n",
    "        results_svm,predictions_svm = perform_evaluation_with_pca(path,svr,param_grid,n_components,method='random',transformation=transformation,data=data)\n",
    "    else:\n",
    "        results_svm,predictions_svm = perform_evaluation(path,svr,param_grid,method='random',transformation=transformation,data=data)\n",
    "\n",
    "    print('********Applying Linear regression with stochastic gradient descent****************')\n",
    "    from sklearn.linear_model import SGDRegressor\n",
    "    param_grid={\n",
    "                'max_iter':[50,100],\n",
    "                'penalty':[None],\n",
    "                'eta0':[0.01,0.1,0.5]\n",
    "               }\n",
    "    sgd_reg = SGDRegressor(random_state=32)\n",
    "    if perform_pca==True:\n",
    "        print(n_components)\n",
    "        print(type(n_components))\n",
    "        results_sgd,predictions_sgd = perform_evaluation_with_pca(path,sgd_reg,param_grid,n_components,transformation=transformation,data=data)\n",
    "    else:\n",
    "        results_sgd, predictions_sgd = perform_evaluation(path,sgd_reg,param_grid,transformation=transformation,data=data)\n",
    "    \n",
    "\n",
    "\n",
    "    ## lasso regression\n",
    "    print('********Applying Lasso Regression****************')\n",
    "    from sklearn.linear_model import Lasso\n",
    "#     alpha_space = np.logspace(-4, 0, 50)\n",
    "    alpha_space = np.logspace(0, 1, 100)\n",
    "    param_grid={'alpha':alpha_space}\n",
    "    lasso = Lasso(random_state=32)\n",
    "    if perform_pca==True:\n",
    "        results_lasso, predictions_lasso = perform_evaluation_with_pca(path,lasso,param_grid,n_components,transformation=transformation,data=data)\n",
    "    else:\n",
    "        results_lasso, predictions_lasso = perform_evaluation(path,lasso,param_grid,transformation=transformation,data=data)\n",
    "    \n",
    "\n",
    "    ## elastic net \n",
    "    print('********Applying Elastic Net Regression****************')\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "#     alpha_space = np.logspace(-4, 0, 50)\n",
    "    alpha_space = np.logspace(0, 2 , 50)\n",
    "    param_grid={'alpha':alpha_space}\n",
    "    enet = ElasticNet(random_state=32)\n",
    "    if perform_pca==True:\n",
    "        results_enet,predictions_enet = perform_evaluation_with_pca(path,enet,param_grid,n_components,transformation=transformation,data=data)\n",
    "    else:\n",
    "        results_enet, predictions_enet = perform_evaluation(path,enet,param_grid,transformation=transformation,data=data)\n",
    "    \n",
    "    \n",
    "    np.random.seed(32)\n",
    "    #MARS\n",
    "    print('********Applying MARS****************')\n",
    "    from pyearth import Earth\n",
    "    max_degree_space=[1]\n",
    "    penalty_space=np.logspace(-1,1,20)\n",
    "    minspan_alpha=np.logspace(-3,1,20)\n",
    "    max_terms=[10,20,25]\n",
    "    param_grid={'max_degree':max_degree_space,\n",
    "        'penalty':penalty_space,\n",
    "        'use_fast':[True],\n",
    "        'max_terms':max_terms\n",
    "               }\n",
    "    mars= Earth()\n",
    "    df_rf=pd.DataFrame(results_rf).T\n",
    "    df_rf['Target']=df_rf.index\n",
    "    df_rf=df_rf.reset_index(drop=True)\n",
    "    df_rf['Algorithm']='Random Forest'\n",
    "    df_rf.set_index(['Algorithm'])\n",
    "\n",
    "    df_svm=pd.DataFrame(results_svm).T\n",
    "    df_svm['Target']=df_svm.index\n",
    "    df_svm=df_svm.reset_index(drop=True)\n",
    "    df_svm['Algorithm']='SVM'\n",
    "    df_svm.set_index(['Algorithm'])\n",
    "\n",
    "    df_sgd=pd.DataFrame(results_sgd).T\n",
    "    df_sgd['Target']=df_sgd.index\n",
    "    df_sgd=df_sgd.reset_index(drop=True)\n",
    "    df_sgd['Algorithm']='Linear regression'\n",
    "    df_sgd.set_index(['Algorithm'])\n",
    "\n",
    "    df_lasso=pd.DataFrame(results_lasso).T\n",
    "    df_lasso['Target']=df_lasso.index\n",
    "    df_lasso=df_lasso.reset_index(drop=True)\n",
    "    df_lasso['Algorithm']='Lasso Regression'\n",
    "    df_lasso.set_index(['Algorithm'])\n",
    "\n",
    "    df_enet=pd.DataFrame(results_enet).T\n",
    "    df_enet['Target']=df_enet.index\n",
    "    df_enet=df_enet.reset_index(drop=True)\n",
    "    df_enet['Algorithm']='Elastic Net'\n",
    "    df_enet.set_index(['Algorithm'])\n",
    "\n",
    "#     concat the df\n",
    "    pd.concat([\n",
    "        df_rf,df_svm,df_sgd,\n",
    "        df_lasso,\n",
    "        df_enet,\n",
    "    ]).to_csv(filename,index=False)\n",
    "    print(\"File saved\")\n",
    "    del df_rf,df_svm,df_sgd,df_lasso,df_enet,\n",
    "    \n",
    "    def createPredictionsTable(predictions):\n",
    "        pq= pd.DataFrame(predictions.get('PQ'))\n",
    "        pq.rename(index=str, columns={\"Original\": \"Original_PQ\", \"Prediction\": \"Prediction_PQ\",'Residuals':'Residuals_PQ'}, inplace=True)\n",
    "        att=pd.DataFrame(predictions.get('ATT'))\n",
    "        att.rename(index=str, columns={\"Original\": \"Original_ATT\", \"Prediction\": \"Prediction_ATT\",'Residuals':'Residuals_ATT'},inplace=True)\n",
    "        df = pd.concat([pq,att],axis=1)\n",
    "        return df\n",
    "    \n",
    "    df_sgd=createPredictionsTable(predictions_sgd)\n",
    "    df_lasso=createPredictionsTable(predictions_lasso)\n",
    "    df_enet=createPredictionsTable(predictions_enet)\n",
    "    df_svm=createPredictionsTable(predictions_svm)\n",
    "    df_rf=createPredictionsTable(predictions_rf)\n",
    "    \n",
    "    if transformation==False and perform_pca==True:\n",
    "        filename=str(data)+'_feature_selection_alltargets_mahalanobis_PCA_'+str(n_components)+'_predictions.xlsx'\n",
    "    elif transformation==False and perform_pca==False:\n",
    "        filename=str(data)+'_feature_selection_alltargets_mahalanobis'+'_predictions.xlsx'\n",
    "    elif transformation==True and perform_pca==True:\n",
    "        filename=str(data)+'_feature_selection_alltargets_mahalanobis_transformed_PCA_'+str(n_components)+'_predictions.xlsx'\n",
    "    else:\n",
    "        filename=str(data)+'_feature_selection_alltargets_mahalanobis_transformed'+'_predictions.xlsx'\n",
    "        \n",
    "    with pd.ExcelWriter(filename) as writer:  # doctest: +SKIP\n",
    "        df_sgd.to_excel(writer, sheet_name='Linear Regression')\n",
    "        df_lasso.to_excel(writer, sheet_name='Lasso Regression')\n",
    "        df_enet.to_excel(writer, sheet_name='Elastic Net')\n",
    "        df_svm.to_excel(writer, sheet_name='SVM')\n",
    "        df_rf.to_excel(writer, sheet_name='Random Forest')\n",
    "    \n",
    "    print('file saved sucessfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study1 original\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on study1 data on original distribution features\n",
    "if __name__=='__main__':\n",
    "    path='/mnt/vdb1/datasets/files_generated/UX/study1_features_data_out_mahalanobis.csv'\n",
    "    filename='Tables/study1_feature_selection_mahalanobis_alltargets.csv'\n",
    "    runAllModels(path,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on study1 data on original distribution after PCA with 95% explained variance PCs\n",
    "if __name__=='__main__':\n",
    "    path='/mnt/vdb1/datasets/files_generated/UX/study1_features_data_out_mahalanobis.csv'\n",
    "    filename='Tables/study1_feature_selection_mahalanobis_alltargets_PCA_0.95PC.csv'\n",
    "    n_components=0.95\n",
    "    runAllModels(path,filename,n_components,perform_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on study1 data on original distribution after PCA with 80% explained variance PCs\n",
    "if __name__=='__main__':\n",
    "    path='/mnt/vdb1/datasets/files_generated/UX/study1_features_data_out_mahalanobis.csv'\n",
    "    filename='Tables/study1_feature_selection_mahalanobis_alltargets_PCA_0.80PC.csv'\n",
    "    n_components=0.80\n",
    "    runAllModels(path,filename,n_components,perform_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on study1 data on original distribution after PCA with 3 PCs\n",
    "if __name__=='__main__':\n",
    "    path='/mnt/vdb1/datasets/files_generated/UX/study1_features_data_out_mahalanobis.csv'\n",
    "    filename='Tables/study1_feature_selection_mahalanobis_alltargets_PCA_3PC.csv'\n",
    "    n_components=3\n",
    "    runAllModels(path,filename,n_components,perform_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study1 Transformation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on study1 data on transformed distribution features\n",
    "if __name__=='__main__':\n",
    "    path='/mnt/vdb1/datasets/files_generated/UX/study1_features_data_out_mahalanobis_transformedDistributions.csv'\n",
    "    filename='Tables/study1_feature_selection_mahalanobis_transformed_alltargets.csv'\n",
    "    runAllModels(path,filename,transformation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on study1 data on transformed distribution after PCA with 95% explained variance PCs\n",
    "if __name__=='__main__':\n",
    "    path='/mnt/vdb1/datasets/files_generated/UX/study1_features_data_out_mahalanobis_transformedDistributions.csv'\n",
    "    filename='Tables/study1_feature_selection_mahalanobis_transformed_alltargets_PCA_0.95PC.csv'\n",
    "    n_components=0.95\n",
    "    runAllModels(path,filename,n_components,transformation=True,perform_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on study1 data on transformed distribution after PCA with 80% explained variance PCs\n",
    "if __name__=='__main__':\n",
    "    path='/mnt/vdb1/datasets/files_generated/UX/study1_features_data_out_mahalanobis_transformedDistributions.csv'\n",
    "    filename='Tables/study1_feature_selection_mahalanobis_transformed_alltargets_PCA_0.80PC.csv'\n",
    "    n_components=0.80\n",
    "    runAllModels(path,filename,n_components,transformation=True,perform_pca=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on study1 data on transformed distribution after PCA with 3 PCs\n",
    "if __name__=='__main__':\n",
    "    path='/mnt/vdb1/datasets/files_generated/UX/study1_features_data_out_mahalanobis_transformedDistributions.csv'\n",
    "    filename='Tables/study1_feature_selection_mahalanobis_transformed_alltargets_PCA_3PC.csv'\n",
    "    n_components=3\n",
    "    runAllModels(path,filename,n_components,transformation=True,perform_pca=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study2 original\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on study2 data on original distribution features\n",
    "if __name__=='__main__':\n",
    "    path='/mnt/vdb1/datasets/files_generated/UX/study2_features_data_out_mahalanobis.csv'\n",
    "    filename='Tables/study2_feature_selection_mahalanobis_alltargets.csv'\n",
    "    runAllModels(path,filename,transformation=False,data='study2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on study2 data on original distribution after PCA with 95% explained variance PCs\n",
    "if __name__=='__main__':\n",
    "    path='/mnt/vdb1/datasets/files_generated/UX/study2_features_data_out_mahalanobis.csv'\n",
    "    filename='Tables/study2_feature_selection_mahalanobis_alltargets_PCA_0.95PC.csv'\n",
    "    n_components=0.95\n",
    "    runAllModels(path,filename,n_components,transformation=False,perform_pca=True,data='study2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on study2 data on original distribution after PCA with 80% explained variance PCs\n",
    "if __name__=='__main__':\n",
    "    path='/mnt/vdb1/datasets/files_generated/UX/study2_features_data_out_mahalanobis.csv'\n",
    "    filename='Tables/study2_feature_selection_mahalanobis_alltargets_PCA_0.80PC.csv'\n",
    "    n_components=0.80\n",
    "    runAllModels(path,filename,n_components,transformation=False,perform_pca=True,data='study2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on study2 data on original distribution after PCA with 3 PCs\n",
    "if __name__=='__main__':\n",
    "    path='../datasets/files_generated/UX/study2_features_data_out_mahalanobis.csv'\n",
    "    filename='Tables/study2_feature_selection_mahalanobis_alltargets_PCA_3PC.csv'\n",
    "    n_components=3\n",
    "    runAllModels(path,filename,n_components,transformation=False,perform_pca=True,data='study2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study2 Transformed\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on study2 data on transformed distribution features\n",
    "if __name__=='__main__':\n",
    "    path='/mnt/vdb1/datasets/files_generated/UX/study2_features_data_out_mahalanobis_transformedDistributions.csv'\n",
    "    filename='Tables/study2_feature_selection_mahalanobis_transformed_alltargets.csv'\n",
    "    runAllModels(path,filename,transformation=True,data='study2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on study2 data on transformed distribution after PCA with 95% explained variance PCs\n",
    "if __name__=='__main__':\n",
    "    path='/mnt/vdb1/datasets/files_generated/UX/study2_features_data_out_mahalanobis_transformedDistributions.csv'\n",
    "    filename='Tables/study2_feature_selection_mahalanobis_transformed_alltargets_PCA_0.95PC.csv'\n",
    "    n_components=0.95\n",
    "    runAllModels(path,filename,n_components,transformation=True,perform_pca=True,data='study2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on study2 data on transformed distribution after PCA with 95% explained variance PCs\n",
    "if __name__=='__main__':\n",
    "    path='/mnt/vdb1/datasets/files_generated/UX/study2_features_data_out_mahalanobis_transformedDistributions.csv'\n",
    "    filename='Tables/study2_feature_selection_mahalanobis_transformed_alltargets_PCA_0.80PC.csv'\n",
    "    n_components=0.80\n",
    "    runAllModels(path,filename,n_components,transformation=True,perform_pca=True,data='study2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on study2 data on transformed distribution after PCA with 3 PCs\n",
    "if __name__=='__main__':\n",
    "    path='/mnt/vdb1/datasets/files_generated/UX/study2_features_data_out_mahalanobis_transformedDistributions.csv'\n",
    "    filename='Tables/study2_feature_selection_mahalanobis_transformed_alltargets_PCA_3PC.csv'\n",
    "    n_components=3\n",
    "    runAllModels(path,filename,n_components,transformation=True,perform_pca=True,data='study2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
